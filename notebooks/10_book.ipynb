{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 10 - Relational Deep Reinforcement Learning\n",
    "### Deep Reinforcement Learning *in Action*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Listing 10.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pytorch using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import torch\n",
    "from torch import nn\n",
    "import torchvision as TV\n",
    "from tqdm.notebook import trange\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('Pytorch using device:', device)\n",
    "\n",
    "try:\n",
    "    mnist_data = TV.datasets.MNIST(\"MNIST/\", train=True, transform=None,\n",
    "                                        target_transform=None, download=False) #A\n",
    "    mnist_test = TV.datasets.MNIST(\"MNIST/\", train=False, transform=None,\n",
    "                                        target_transform=None, download=False) #B\n",
    "except:\n",
    "    mnist_data = TV.datasets.MNIST(\"MNIST/\", train=True, transform=None,\n",
    "                                        target_transform=None, download=True) #A\n",
    "    mnist_test = TV.datasets.MNIST(\"MNIST/\", train=False, transform=None,\n",
    "                                        target_transform=None, download=True) #B\n",
    "\n",
    "def add_spots(x,m=20,std=5,val=1): #C\n",
    "    mask = torch.zeros(x.shape)\n",
    "    N = int(m + std * np.abs(np.random.randn()))\n",
    "    ids = np.random.randint(np.prod(x.shape),size=N)\n",
    "    mask.view(-1)[ids] = val\n",
    "    return torch.clamp(x + mask,0,1)\n",
    "\n",
    "def prepare_images(xt,maxtrans=6,rot=5,noise=10): #D\n",
    "    out = torch.zeros(xt.shape)\n",
    "    for i in range(xt.shape[0]):\n",
    "        img = xt[i].unsqueeze(dim=0)\n",
    "        img = TV.transforms.functional.to_pil_image(img)\n",
    "        rand_rot = np.random.randint(-1*rot,rot,1) if rot > 0 else 0\n",
    "        xtrans,ytrans = np.random.randint(-maxtrans,maxtrans,2)\n",
    "        img = TV.transforms.functional.affine(img=img, angle=int(rand_rot[0]), \n",
    "                                              translate=(xtrans,ytrans),scale=1,shear=0)\n",
    "        img = TV.transforms.functional.to_tensor(img).squeeze()\n",
    "        if noise > 0:\n",
    "            img = add_spots(img,m=noise)\n",
    "        maxval = img.view(-1).max()\n",
    "        if maxval > 0:\n",
    "            img = img.float() / maxval\n",
    "        else:\n",
    "            img = img.float()\n",
    "        out[i] = img\n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Listing 10.2/10.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RelationalModule(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(RelationalModule, self).__init__()\n",
    "        self.ch_in = 1\n",
    "        self.conv1_ch = 16 #A\n",
    "        self.conv2_ch = 20\n",
    "        self.conv3_ch = 24\n",
    "        self.conv4_ch = 30\n",
    "        self.H = 28 #B\n",
    "        self.W = 28\n",
    "        self.node_size = 36 #C\n",
    "        self.lin_hid = 100\n",
    "        self.out_dim = 10\n",
    "        self.sp_coord_dim = 2\n",
    "        self.N = int(16**2) #D\n",
    "\n",
    "        self.conv1 = nn.Conv2d(self.ch_in,self.conv1_ch,kernel_size=(4,4))\n",
    "        self.conv2 = nn.Conv2d(self.conv1_ch,self.conv2_ch,kernel_size=(4,4))\n",
    "        self.conv3 = nn.Conv2d(self.conv2_ch,self.conv3_ch,kernel_size=(4,4))\n",
    "        self.conv4 = nn.Conv2d(self.conv3_ch,self.conv4_ch,kernel_size=(4,4))\n",
    "        \n",
    "        self.proj_shape = (self.conv4_ch+self.sp_coord_dim,self.node_size) #E\n",
    "        self.k_proj = nn.Linear(*self.proj_shape)\n",
    "        self.q_proj = nn.Linear(*self.proj_shape)\n",
    "        self.v_proj = nn.Linear(*self.proj_shape)\n",
    "        \n",
    "        self.norm_shape = (self.N,self.node_size)\n",
    "        self.k_norm = nn.LayerNorm(self.norm_shape, elementwise_affine=True) #F\n",
    "        self.q_norm = nn.LayerNorm(self.norm_shape, elementwise_affine=True)\n",
    "        self.v_norm = nn.LayerNorm(self.norm_shape, elementwise_affine=True)\n",
    "        \n",
    "        self.linear1 = nn.Linear(self.node_size, self.node_size)\n",
    "        self.norm1 = nn.LayerNorm([self.N,self.node_size], elementwise_affine=False)\n",
    "        self.linear2 = nn.Linear(self.node_size, self.out_dim)\n",
    "\n",
    "    def forward(self,x):\n",
    "            N, Cin, H, W = x.shape\n",
    "            x = self.conv1(x) \n",
    "            x = torch.relu(x)\n",
    "            x = self.conv2(x) \n",
    "            x = x.squeeze() \n",
    "            x = torch.relu(x) \n",
    "            x = self.conv3(x)\n",
    "            x = torch.relu(x)\n",
    "            x = self.conv4(x)\n",
    "            x = torch.relu(x)\n",
    "\n",
    "            _,_,cH,cW = x.shape\n",
    "            xcoords = torch.arange(cW).repeat(cH,1).float() / cW #G\n",
    "            ycoords = torch.arange(cH).repeat(cW,1).transpose(1,0).float() / cH\n",
    "            spatial_coords = torch.stack([xcoords,ycoords],dim=0)\n",
    "            spatial_coords = spatial_coords.unsqueeze(dim=0)\n",
    "            spatial_coords = spatial_coords.repeat(N,1,1,1) \n",
    "            x = torch.cat([x,spatial_coords],dim=1)\n",
    "            x = x.permute(0,2,3,1)\n",
    "            x = x.flatten(1,2)\n",
    "\n",
    "            K = self.k_proj(x) #H\n",
    "            K = self.k_norm(K) \n",
    "\n",
    "            Q = self.q_proj(x)\n",
    "            Q = self.q_norm(Q) \n",
    "\n",
    "            V = self.v_proj(x)\n",
    "            V = self.v_norm(V) \n",
    "            A = torch.einsum('bfe,bge->bfg',Q,K) #I\n",
    "            A = A / np.sqrt(self.node_size)\n",
    "            A = torch.nn.functional.softmax(A,dim=2) \n",
    "            with torch.no_grad():\n",
    "                self.att_map = A.clone()\n",
    "            E = torch.einsum('bfc,bcd->bfd',A,V) #J\n",
    "            E = self.linear1(E)\n",
    "            E = torch.relu(E)\n",
    "            E = self.norm1(E)  \n",
    "            E = E.max(dim=1)[0]\n",
    "            y = self.linear2(E)  \n",
    "            y = torch.nn.functional.log_softmax(y,dim=1)\n",
    "            return y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Listing 10.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "397259b6d7f643339cce2d5142d1ebd2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "agent = RelationalModule() #A\n",
    "epochs = 1000\n",
    "batch_size=300\n",
    "lr = 1e-3\n",
    "opt = torch.optim.Adam(params=agent.parameters(),lr=lr)\n",
    "lossfn = nn.NLLLoss()\n",
    "for i in trange(epochs):\n",
    "    opt.zero_grad()\n",
    "    batch_ids = np.random.randint(0,60000,size=batch_size) #B\n",
    "    xt = mnist_data.data[batch_ids].detach()\n",
    "    xt = prepare_images(xt,rot=30).unsqueeze(dim=1) #C\n",
    "    yt = mnist_data.targets[batch_ids].detach()\n",
    "    pred = agent(xt)\n",
    "    pred_labels = torch.argmax(pred,dim=1) #D\n",
    "    acc_ = 100.0 * (pred_labels == yt).sum() / batch_size #E\n",
    "    correct = torch.zeros(batch_size,10)\n",
    "    rows = torch.arange(batch_size).long()\n",
    "    correct[[rows,yt.detach().long()]] = 1.\n",
    "    loss = lossfn(pred,yt)\n",
    "    loss.backward()\n",
    "    opt.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Listing 10.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jack/.cache/pypoetry/virtualenvs/reinforcement-learning-notebooks-_NpvE3mH-py3.8/lib/python3.8/site-packages/torchvision/datasets/mnist.py:67: UserWarning: test_data has been renamed data\n",
      "  warnings.warn(\"test_data has been renamed data\")\n",
      "/home/jack/.cache/pypoetry/virtualenvs/reinforcement-learning-notebooks-_NpvE3mH-py3.8/lib/python3.8/site-packages/torchvision/datasets/mnist.py:57: UserWarning: test_labels has been renamed targets\n",
      "  warnings.warn(\"test_labels has been renamed targets\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.9340)\n"
     ]
    }
   ],
   "source": [
    "def test_acc(model,batch_size=500):\n",
    "    acc = 0.\n",
    "    batch_ids = np.random.randint(0,10000,size=batch_size)\n",
    "    xt = mnist_test.data[batch_ids].detach()\n",
    "    xt = prepare_images(xt,maxtrans=6,rot=30,noise=10).unsqueeze(dim=1)\n",
    "    yt = mnist_test.targets[batch_ids].detach()\n",
    "    preds = model(xt)\n",
    "    pred_ind = torch.argmax(preds.detach(),dim=1)\n",
    "    acc = (pred_ind == yt).sum().float() / batch_size\n",
    "    return acc, xt, yt\n",
    "\n",
    "acc2, xt2, yt2 = test_acc(agent)\n",
    "print(acc2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f8428360e50>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(agent.att_map[0].max(dim=0)[0].view(16,16))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Listing 10.6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1,10,kernel_size=(4,4)) #A\n",
    "        self.conv2 = nn.Conv2d(10,16,kernel_size=(4,4))\n",
    "        self.conv3 = nn.Conv2d(16,24,kernel_size=(4,4))\n",
    "        self.conv4 = nn.Conv2d(24,32,kernel_size=(4,4))\n",
    "        self.maxpool1 = nn.MaxPool2d(kernel_size=(2,2)) #B\n",
    "        self.conv5 = nn.Conv2d(32,64,kernel_size=(4,4))\n",
    "        self.lin1 = nn.Linear(256,128)\n",
    "        self.out = nn.Linear(128,10) #C\n",
    "    def forward(self,x):\n",
    "        x = self.conv1(x)\n",
    "        x = nn.functional.relu(x)\n",
    "        x = self.conv2(x)\n",
    "        x = nn.functional.relu(x)\n",
    "        x = self.maxpool1(x)\n",
    "        x = self.conv3(x)\n",
    "        x = nn.functional.relu(x)\n",
    "        x = self.conv4(x)\n",
    "        x = nn.functional.relu(x)\n",
    "        x = self.conv5(x)\n",
    "        x = nn.functional.relu(x)\n",
    "        x = x.flatten(start_dim=1)\n",
    "        x = self.lin1(x)\n",
    "        x = nn.functional.relu(x)\n",
    "        x = self.out(x)\n",
    "        x = nn.functional.log_softmax(x,dim=1) #D\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 3, 7, 7])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from einops import rearrange\n",
    "x = torch.randn(5,7,7,3)\n",
    "rearrange(x, \"batch h w c -> batch c h w\").shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Listing 10.7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadRelationalModule(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MultiHeadRelationalModule, self).__init__()\n",
    "        self.conv1_ch = 16 \n",
    "        self.conv2_ch = 20\n",
    "        self.conv3_ch = 24\n",
    "        self.conv4_ch = 30\n",
    "        self.H = 28\n",
    "        self.W = 28\n",
    "        self.node_size = 64\n",
    "        self.lin_hid = 100\n",
    "        self.out_dim = 5\n",
    "        self.ch_in = 3\n",
    "        self.sp_coord_dim = 2\n",
    "        self.N = int(7**2)\n",
    "        self.n_heads = 3\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(self.ch_in,self.conv1_ch,kernel_size=(1,1),padding=0) #A\n",
    "        self.conv2 = nn.Conv2d(self.conv1_ch,self.conv2_ch,kernel_size=(1,1),padding=0)\n",
    "        self.proj_shape = (self.conv2_ch+self.sp_coord_dim,self.n_heads * self.node_size)\n",
    "        self.k_proj = nn.Linear(*self.proj_shape)\n",
    "        self.q_proj = nn.Linear(*self.proj_shape)\n",
    "        self.v_proj = nn.Linear(*self.proj_shape)\n",
    "\n",
    "        self.k_lin = nn.Linear(self.node_size,self.N) #B\n",
    "        self.q_lin = nn.Linear(self.node_size,self.N)\n",
    "        self.a_lin = nn.Linear(self.N,self.N)\n",
    "        \n",
    "        self.node_shape = (self.n_heads, self.N,self.node_size)\n",
    "        self.k_norm = nn.LayerNorm(self.node_shape, elementwise_affine=True)\n",
    "        self.q_norm = nn.LayerNorm(self.node_shape, elementwise_affine=True)\n",
    "        self.v_norm = nn.LayerNorm(self.node_shape, elementwise_affine=True)\n",
    "        \n",
    "        self.linear1 = nn.Linear(self.n_heads * self.node_size, self.node_size)\n",
    "        self.norm1 = nn.LayerNorm([self.N,self.node_size], elementwise_affine=False)\n",
    "        self.linear2 = nn.Linear(self.node_size, self.out_dim)\n",
    "    \n",
    "    def forward(self,x):\n",
    "        N, Cin, H, W = x.shape\n",
    "        x = self.conv1(x) \n",
    "        x = torch.relu(x)\n",
    "        x = self.conv2(x) \n",
    "        x = torch.relu(x) \n",
    "        with torch.no_grad(): \n",
    "            self.conv_map = x.clone() #C\n",
    "        _,_,cH,cW = x.shape\n",
    "        xcoords = torch.arange(cW).repeat(cH,1).float() / cW\n",
    "        ycoords = torch.arange(cH).repeat(cW,1).transpose(1,0).float() / cH\n",
    "        spatial_coords = torch.stack([xcoords,ycoords],dim=0)\n",
    "        spatial_coords = spatial_coords.unsqueeze(dim=0)\n",
    "        spatial_coords = spatial_coords.repeat(N,1,1,1)\n",
    "        x = torch.cat([x,spatial_coords],dim=1)\n",
    "        x = x.permute(0,2,3,1)\n",
    "        x = x.flatten(1,2)\n",
    "        \n",
    "        K = rearrange(self.k_proj(x), \"b n (head d) -> b head n d\", head=self.n_heads)\n",
    "        K = self.k_norm(K) \n",
    "        \n",
    "        Q = rearrange(self.q_proj(x), \"b n (head d) -> b head n d\", head=self.n_heads)\n",
    "        Q = self.q_norm(Q) \n",
    "        \n",
    "        V = rearrange(self.v_proj(x), \"b n (head d) -> b head n d\", head=self.n_heads)\n",
    "        V = self.v_norm(V) \n",
    "        A = torch.nn.functional.elu(self.q_lin(Q) + self.k_lin(K)) #D\n",
    "        A = self.a_lin(A)\n",
    "        A = torch.nn.functional.softmax(A,dim=3) \n",
    "        with torch.no_grad():\n",
    "            self.att_map = A.clone() #E\n",
    "        E = torch.einsum('bhfc,bhcd->bhfd',A,V) #F\n",
    "        E = rearrange(E, 'b head n d -> b n (head d)')\n",
    "        E = self.linear1(E)\n",
    "        E = torch.relu(E)\n",
    "        E = self.norm1(E)\n",
    "        E = E.max(dim=1)[0]\n",
    "        y = self.linear2(E)\n",
    "        y = torch.nn.functional.elu(y)\n",
    "        return y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Listing 10.8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "from gym_minigrid.minigrid import *\n",
    "from gym_minigrid.wrappers import FullyObsWrapper, ImgObsWrapper\n",
    "from skimage.transform import resize\n",
    "\n",
    "def prepare_state(x): #A\n",
    "    ns = torch.from_numpy(x).float().permute(2,0,1).unsqueeze(dim=0)#\n",
    "    maxv = ns.flatten().max()\n",
    "    ns = ns / maxv\n",
    "    return ns\n",
    "\n",
    "def get_minibatch(replay,size): #B\n",
    "    batch_ids = np.random.randint(0,len(replay),size)\n",
    "    batch = [replay[x] for x in batch_ids] #list of tuples\n",
    "    state_batch = torch.cat([s for (s,a,r,s2,d) in batch],)\n",
    "    action_batch = torch.Tensor([a for (s,a,r,s2,d) in batch]).long()\n",
    "    reward_batch = torch.Tensor([r for (s,a,r,s2,d) in batch])\n",
    "    state2_batch = torch.cat([s2 for (s,a,r,s2,d) in batch],dim=0)\n",
    "    done_batch = torch.Tensor([d for (s,a,r,s2,d) in batch])\n",
    "    return state_batch,action_batch,reward_batch,state2_batch, done_batch\n",
    "\n",
    "def get_qtarget_ddqn(qvals,r,df,done): #C\n",
    "    targets = r + (1-done) * df * qvals\n",
    "    return targets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Listing 10.9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lossfn(pred,targets,actions): #A\n",
    "    loss = torch.mean(torch.pow(\\\n",
    "                                targets.detach() -\\\n",
    "                                pred.gather(dim=1,index=actions.unsqueeze(dim=1)).squeeze()\\\n",
    "                                ,2),dim=0)\n",
    "    return loss\n",
    "  \n",
    "def update_replay(replay,exp,replay_size): #B\n",
    "    r = exp[2]\n",
    "    N = 1\n",
    "    if r > 0:\n",
    "        N = 50\n",
    "    for i in range(N):\n",
    "        replay.append(exp)\n",
    "    return replay\n",
    "\n",
    "action_map = { #C\n",
    "    0:0, \n",
    "    1:1,\n",
    "    2:2,\n",
    "    3:3,\n",
    "    4:5,\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Listing 10.10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e02eb1df712f4c19b84a18eceb1c80f5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from collections import deque\n",
    "env = ImgObsWrapper(gym.make('MiniGrid-DoorKey-5x5-v0')) #A\n",
    "state = prepare_state(env.reset()) \n",
    "GWagent = MultiHeadRelationalModule() #B\n",
    "Tnet = MultiHeadRelationalModule() #C\n",
    "maxsteps = 400 #D\n",
    "env.max_steps = maxsteps\n",
    "env.env.max_steps = maxsteps\n",
    "\n",
    "epochs = 50000\n",
    "replay_size = 9000\n",
    "batch_size = 50\n",
    "lr = 0.0005\n",
    "gamma = 0.99\n",
    "replay = deque(maxlen=replay_size) #E\n",
    "opt = torch.optim.Adam(params=GWagent.parameters(),lr=lr)\n",
    "eps = 0.5\n",
    "update_freq = 100\n",
    "for i in trange(epochs):\n",
    "    pred = GWagent(state)\n",
    "    action = int(torch.argmax(pred).detach().numpy())\n",
    "    if np.random.rand() < eps: #F\n",
    "        action = int(torch.randint(0,5,size=(1,)).squeeze())\n",
    "    action_d = action_map[action]\n",
    "    state2, reward, done, info = env.step(action_d)\n",
    "    reward = -0.01 if reward == 0 else reward #G\n",
    "    state2 = prepare_state(state2)\n",
    "    exp = (state,action,reward,state2,done)\n",
    "    \n",
    "    replay = update_replay(replay,exp,replay_size)\n",
    "    if done:\n",
    "        state = prepare_state(env.reset())\n",
    "    else:\n",
    "        state = state2\n",
    "    if len(replay) > batch_size:\n",
    "        \n",
    "        opt.zero_grad()\n",
    "        \n",
    "        state_batch,action_batch,reward_batch,state2_batch,done_batch = get_minibatch(replay,batch_size)\n",
    "        \n",
    "        q_pred = GWagent(state_batch).cpu()\n",
    "        astar = torch.argmax(q_pred,dim=1)\n",
    "        qs = Tnet(state2_batch).gather(dim=1,index=astar.unsqueeze(dim=1)).squeeze()\n",
    "        \n",
    "        targets = get_qtarget_ddqn(qs.detach(),reward_batch.detach(),gamma,done_batch)\n",
    "        \n",
    "        loss = lossfn(q_pred,targets.detach(),action_batch)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(GWagent.parameters(), max_norm=1.0) #H\n",
    "        opt.step()\n",
    "    if i % update_freq == 0: #I\n",
    "        Tnet.load_state_dict(GWagent.state_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f84227c7e50>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "state_ = env.reset()\n",
    "state = prepare_state(state_)\n",
    "GWagent(state)\n",
    "plt.imshow(env.render('rgb_array'))\n",
    "plt.imshow(state[0].permute(1,2,0).detach().numpy())\n",
    "head, node = 2, 26\n",
    "plt.imshow(GWagent.att_map[0][head][node].view(7,7))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deep-rl-notebooks-poetry",
   "language": "python",
   "name": "deep-rl-notebooks-poetry"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
